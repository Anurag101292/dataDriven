🛠️ Java Implementation – LRU Caching in a Mock Server
📁 Use case:
You’re mocking a /getUserProfile endpoint.

👇 LRU Cache using LinkedHashMap:

public class LRUCache<K, V> extends LinkedHashMap<K, V> {
    private final int capacity;

    public LRUCache(int capacity) {
        super(capacity, 0.75f, true); // accessOrder = true
        this.capacity = capacity;
    }

    @Override
    protected boolean removeEldestEntry(Map.Entry<K, V> eldest) {
        return size() > capacity;
    }
}
🔄 Integrate into Mock Controller

@RestController
public class MockApiController {

    private final LRUCache<String, String> responseCache = new LRUCache<>(100);

    @PostMapping("/getUserProfile")
    public ResponseEntity<String> getUserProfile(@RequestBody Map<String, Object> request) {
        String cacheKey = generateCacheKey(request);

        // 1. Return from cache if available
        if (responseCache.containsKey(cacheKey)) {
            return ResponseEntity.ok(responseCache.get(cacheKey));
        }

        // 2. Otherwise, create mock response
        String response = createMockResponse(request);

        // 3. Store it in cache
        responseCache.put(cacheKey, response);

        return ResponseEntity.ok(response);
    }

    private String generateCacheKey(Map<String, Object> request) {
        return request.toString(); // You can use better hashing/serialization here
    }

    private String createMockResponse(Map<String, Object> request) {
        // Simulate mock logic
        return "{ \"id\": \"" + request.get("id") + "\", \"status\": \"ACTIVE\" }";
    }
}
✅ Benefits of Adding LRU Cache
Feature	Without Cache	With LRU Cache
CPU Utilization	High (recompute every time)	Low (reuse response)
Response Time	Slower for repeated inputs	Faster (almost instant)
Memory Usage	Grows uncontrolled	Capped (e.g., 100 entries)
Test Reliability	May delay under load	Stable under load

In our QA test ecosystem, we implemented an LRU cache in the mock server to reduce redundant processing of repeated API requests. 
This optimized the performance of over 1000 automated scenarios by 20–30%, stabilized parallel executions,
and avoided mock response bottlenecks in our CI pipeline.
